{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VoxelMorph on neurite-OASIS demo\n",
        "Quick demo on how to train voxelmorph on [neurite-OASIS](https://github.com/adalca/medical-datasets/blob/master/neurite-oasis.md) release using provided `train.py` file."
      ],
      "metadata": {
        "id": "EI1qNmBt75Zc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4mKxwJiK3X8r",
        "outputId": "afe5ca3b-48dd-457c-a7d1-bd93de253c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'voxelmorph' already exists and is not an empty directory.\n",
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Successfully uninstalled tensorflow-2.17.0\n",
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.11.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.0)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.0)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.64.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorflow"
                ]
              },
              "id": "f79a8b18039649bc9779af30f8805d3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neurite in /usr/local/lib/python3.10/dist-packages (0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neurite) (24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from neurite) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from neurite) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from neurite) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from neurite) (4.66.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from neurite) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from neurite) (1.3.2)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from neurite) (5.0.1)\n",
            "Requirement already satisfied: pystrum>=0.2 in /usr/local/lib/python3.10/dist-packages (from neurite) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurite) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurite) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurite) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurite) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurite) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurite) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurite) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->neurite) (71.0.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->neurite) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->neurite) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "# install voxelmorph\n",
        "!git clone https://github.com/voxelmorph/voxelmorph.git # getting vxm from git to use scripts\n",
        "import tensorflow as tf\n",
        "if tf.__version__ != '2.8.0':\n",
        "  !pip uninstall -y tensorflow\n",
        "  !pip install tensorflow==2.8.0\n",
        "!pip install neurite # dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import voxelmorph\n",
        "import sys\n",
        "sys.path.append('voxelmorph')\n",
        "import voxelmorph as vxm"
      ],
      "metadata": {
        "id": "7dC5zJ605UXz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get neurite-OASIS data\n",
        "!wget https://surfer.nmr.mgh.harvard.edu/ftp/data/neurite/data/neurite-oasis.2d.v1.0.tar\n",
        "!mkdir OASIS\n",
        "!tar xf neurite-oasis.2d.v1.0.tar --directory 'OASIS'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_IKPFaf3d3F",
        "outputId": "49eab71f-bfa2-4976-bb1b-eb2cac577313"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-02 15:43:17--  https://surfer.nmr.mgh.harvard.edu/ftp/data/neurite/data/neurite-oasis.2d.v1.0.tar\n",
            "Resolving surfer.nmr.mgh.harvard.edu (surfer.nmr.mgh.harvard.edu)... 132.183.1.43\n",
            "Connecting to surfer.nmr.mgh.harvard.edu (surfer.nmr.mgh.harvard.edu)|132.183.1.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24903680 (24M) [application/x-tar]\n",
            "Saving to: ‘neurite-oasis.2d.v1.0.tar’\n",
            "\n",
            "neurite-oasis.2d.v1 100%[===================>]  23.75M  22.8MB/s    in 1.0s    \n",
            "\n",
            "2024-08-02 15:43:18 (22.8 MB/s) - ‘neurite-oasis.2d.v1.0.tar’ saved [24903680/24903680]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write a list of OASIS subjects to a training file\n",
        "import pathlib\n",
        "path = pathlib.Path('/content/OASIS')\n",
        "subj_lst_m = [str(f/'slice_norm.nii.gz') for f in path.iterdir() if str(f).endswith('MR1')]\n",
        "with open('train_list.txt','w') as tfile:\n",
        "\ttfile.write('\\n'.join(subj_lst_m))"
      ],
      "metadata": {
        "id": "nx7Fq0mw6YZv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run training via train.py (here just 10 epochs for demonstration)\n",
        "%run -i /content/voxelmorph/scripts/tf/train.py --img-list 'train_list.txt'  --epochs 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y7g6mJZ38iT",
        "outputId": "805eb7e8-97b8-450c-d0a2-7c90b8272599"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 47s 371ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0072 - vxm_dense_transformer_loss: 0.0072 - vxm_dense_flow_resize_loss: 1.1447e-04\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 36s 360ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0069 - vxm_dense_transformer_loss: 0.0069 - vxm_dense_flow_resize_loss: 2.9165e-04\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 38s 377ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0061 - vxm_dense_transformer_loss: 0.0060 - vxm_dense_flow_resize_loss: 0.0115\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 36s 364ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0052 - vxm_dense_transformer_loss: 0.0048 - vxm_dense_flow_resize_loss: 0.0408\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 35s 353ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0046 - vxm_dense_transformer_loss: 0.0041 - vxm_dense_flow_resize_loss: 0.0467\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 40s 401ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0042 - vxm_dense_transformer_loss: 0.0037 - vxm_dense_flow_resize_loss: 0.0568\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 37s 370ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0042 - vxm_dense_transformer_loss: 0.0035 - vxm_dense_flow_resize_loss: 0.0628\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 38s 381ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0042 - vxm_dense_transformer_loss: 0.0035 - vxm_dense_flow_resize_loss: 0.0640\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 36s 360ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0039 - vxm_dense_transformer_loss: 0.0033 - vxm_dense_flow_resize_loss: 0.0655\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 36s 359ms/step - batch: 49.5000 - size: 1.0000 - loss: 0.0038 - vxm_dense_transformer_loss: 0.0031 - vxm_dense_flow_resize_loss: 0.0655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"\n",
        "Example script to train a VoxelMorph model.\n",
        "\n",
        "You will likely have to customize this script slightly to accommodate your own data. All images\n",
        "should be appropriately cropped and scaled to values between 0 and 1.\n",
        "\n",
        "If an atlas file is provided with the --atlas flag, then scan-to-atlas training is performed.\n",
        "Otherwise, registration will be scan-to-scan.\n",
        "\n",
        "If you use this code, please cite the following, and read function docs for further info/citations.\n",
        "\n",
        "    VoxelMorph: A Learning Framework for Deformable Medical Image Registration\n",
        "    G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, A.V. Dalca.\n",
        "    IEEE TMI: Transactions on Medical Imaging. 38(8). pp 1788-1800. 2019.\n",
        "\n",
        "    or\n",
        "\n",
        "    Unsupervised Learning for Probabilistic Diffeomorphic Registration for Images and Surfaces\n",
        "    A.V. Dalca, G. Balakrishnan, J. Guttag, M.R. Sabuncu.\n",
        "    MedIA: Medical Image Analysis. (57). pp 226-236, 2019\n",
        "\n",
        "Copyright 2020 Adrian V. Dalca\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in\n",
        "compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is\n",
        "distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
        "implied. See the License for the specific language governing permissions and limitations under the\n",
        "License.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import voxelmorph as vxm\n",
        "\n",
        "\n",
        "# disable eager execution\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.experimental.output_all_intermediates(True) # https://github.com/tensorflow/tensorflow/issues/54458\n",
        "\n",
        "# parse the commandline\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# data organization parameters\n",
        "parser.add_argument('--img-prefix', help='optional input image file prefix')\n",
        "parser.add_argument('--img-suffix', help='optional input image file suffix')\n",
        "parser.add_argument('--atlas', help='optional atlas filename')\n",
        "parser.add_argument('--model-dir', default='models',\n",
        "                    help='model output directory (default: models)')\n",
        "parser.add_argument('--multichannel', action='store_true',\n",
        "                    help='specify that data has multiple channels')\n",
        "\n",
        "# training parameters\n",
        "parser.add_argument('--gpu', default='0', help='GPU ID numbers (default: 0)')\n",
        "parser.add_argument('--batch-size', type=int, default=1, help='batch size (default: 1)')\n",
        "parser.add_argument('--epochs', type=int, default=1500,\n",
        "                    help='number of training epochs (default: 1500)')\n",
        "parser.add_argument('--steps-per-epoch', type=int, default=100,\n",
        "                    help='frequency of model saves (default: 100)')\n",
        "parser.add_argument('--load-weights', help='optional weights file to initialize with')\n",
        "parser.add_argument('--initial-epoch', type=int, default=0,\n",
        "                    help='initial epoch number (default: 0)')\n",
        "parser.add_argument('--lr', type=float, default=1e-4, help='learning rate (default: 1e-4)')\n",
        "\n",
        "# network architecture parameters\n",
        "parser.add_argument('--enc', type=int, nargs='+',\n",
        "                    help='list of unet encoder filters (default: 16 32 32 32)')\n",
        "parser.add_argument('--dec', type=int, nargs='+',\n",
        "                    help='list of unet decorder filters (default: 32 32 32 32 32 16 16)')\n",
        "parser.add_argument('--int-steps', type=int, default=7,\n",
        "                    help='number of integration steps (default: 7)')\n",
        "parser.add_argument('--int-downsize', type=int, default=2,\n",
        "                    help='flow downsample factor for integration (default: 2)')\n",
        "parser.add_argument('--use-probs', action='store_true', help='enable probabilities')\n",
        "parser.add_argument('--bidir', action='store_true', help='enable bidirectional cost function')\n",
        "\n",
        "# loss hyperparameters\n",
        "parser.add_argument('--image-loss', default='mse',\n",
        "                    help='image reconstruction loss - can be mse or ncc (default: mse)')\n",
        "parser.add_argument('--lambda', type=float, dest='lambda_weight', default=0.01,\n",
        "                    help='weight of gradient or KL loss (default: 0.01)')\n",
        "parser.add_argument('--kl-lambda', type=float, default=10,\n",
        "                    help='prior lambda regularization for KL loss (default: 10)')\n",
        "parser.add_argument('--legacy-image-sigma', dest='image_sigma', type=float, default=1.0,\n",
        "                    help='image noise parameter for miccai 2018 network (recommended value is 0.02 when --use-probs is enabled)')  # nopep8\n",
        "args = parser.parse_args()\n",
        "\n",
        "# load and prepare training data\n",
        "train_files = vxm.py.utils.read_file_list(list(''), prefix=args.img_prefix,\n",
        "                                          suffix=args.img_suffix)\n",
        "assert len(train_files) > 0, 'Could not find any training data.'\n"
      ],
      "metadata": {
        "id": "KzE9dmml3TUe",
        "outputId": "d6fdfaa4-7e6d-4693-f10f-31a3aee8f09c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] --img-list IMG_LIST [--img-prefix IMG_PREFIX]\n",
            "                                [--img-suffix IMG_SUFFIX] [--atlas ATLAS] [--model-dir MODEL_DIR]\n",
            "                                [--multichannel] [--gpu GPU] [--batch-size BATCH_SIZE]\n",
            "                                [--epochs EPOCHS] [--steps-per-epoch STEPS_PER_EPOCH]\n",
            "                                [--load-weights LOAD_WEIGHTS] [--initial-epoch INITIAL_EPOCH]\n",
            "                                [--lr LR] [--enc ENC [ENC ...]] [--dec DEC [DEC ...]]\n",
            "                                [--int-steps INT_STEPS] [--int-downsize INT_DOWNSIZE]\n",
            "                                [--use-probs] [--bidir] [--image-loss IMAGE_LOSS]\n",
            "                                [--lambda LAMBDA_WEIGHT] [--kl-lambda KL_LAMBDA]\n",
            "                                [--legacy-image-sigma IMAGE_SIGMA]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --img-list\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# no need to append an extra feature axis if data is multichannel\n",
        "add_feat_axis = not args.multichannel\n",
        "\n",
        "if args.atlas:\n",
        "    # scan-to-atlas generator\n",
        "    atlas = vxm.py.utils.load_volfile(args.atlas, np_var='vol',\n",
        "                                      add_batch_axis=True, add_feat_axis=add_feat_axis)\n",
        "    generator = vxm.generators.scan_to_atlas(train_files, atlas,\n",
        "                                             batch_size=args.batch_size,\n",
        "                                             bidir=args.bidir,\n",
        "                                             add_feat_axis=add_feat_axis)\n",
        "else:\n",
        "    # scan-to-scan generator\n",
        "    generator = vxm.generators.scan_to_scan(\n",
        "        train_files, batch_size=args.batch_size, bidir=args.bidir, add_feat_axis=add_feat_axis)\n",
        "\n",
        "# extract shape and number of features from sampled input\n",
        "sample_shape = next(generator)[0][0].shape\n",
        "inshape = sample_shape[1:-1]\n",
        "nfeats = sample_shape[-1]\n",
        "\n",
        "# prepare model folder\n",
        "model_dir = args.model_dir\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# tensorflow device handling\n",
        "device, nb_devices = vxm.tf.utils.setup_device(args.gpu)\n",
        "assert np.mod(args.batch_size, nb_devices) == 0, \\\n",
        "    'Batch size (%d) should be a multiple of the nr of gpus (%d)' % (args.batch_size, nb_devices)\n",
        "\n",
        "# unet architecture\n",
        "enc_nf = args.enc if args.enc else [16, 32, 32, 32]\n",
        "dec_nf = args.dec if args.dec else [32, 32, 32, 32, 32, 16, 16]\n",
        "\n",
        "# prepare model checkpoint save path\n",
        "save_filename = os.path.join(model_dir, '{epoch:04d}.h5')\n",
        "\n",
        "# build the model\n",
        "model = vxm.networks.VxmDense(\n",
        "    inshape=inshape,\n",
        "    nb_unet_features=[enc_nf, dec_nf],\n",
        "    bidir=args.bidir,\n",
        "    use_probs=args.use_probs,\n",
        "    int_steps=args.int_steps,\n",
        "    int_resolution=args.int_downsize,\n",
        "    src_feats=nfeats,\n",
        "    trg_feats=nfeats\n",
        ")\n",
        "\n",
        "# load initial weights (if provided)\n",
        "if args.load_weights:\n",
        "    model.load_weights(args.load_weights)\n",
        "\n",
        "# prepare image loss\n",
        "if args.image_loss == 'ncc':\n",
        "    image_loss_func = vxm.losses.NCC().loss\n",
        "elif args.image_loss == 'mse':\n",
        "    image_loss_func = vxm.losses.MSE(args.image_sigma).loss\n",
        "else:\n",
        "    raise ValueError('Image loss should be \"mse\" or \"ncc\", but found \"%s\"' % args.image_loss)\n",
        "\n",
        "# need two image loss functions if bidirectional\n",
        "if args.bidir:\n",
        "    losses = [image_loss_func, image_loss_func]\n",
        "    weights = [0.5, 0.5]\n",
        "else:\n",
        "    losses = [image_loss_func]\n",
        "    weights = [1]\n",
        "\n",
        "# prepare deformation loss\n",
        "if args.use_probs:\n",
        "    flow_shape = model.outputs[-1].shape[1:-1]\n",
        "    losses += [vxm.losses.KL(args.kl_lambda, flow_shape).loss]\n",
        "else:\n",
        "    losses += [vxm.losses.Grad('l2', loss_mult=args.int_downsize).loss]\n",
        "\n",
        "weights += [args.lambda_weight]\n",
        "\n",
        "# multi-gpu support\n",
        "if nb_devices > 1:\n",
        "    save_callback = vxm.networks.ModelCheckpointParallel(save_filename)\n",
        "    model = tf.keras.utils.multi_gpu_model(model, gpus=nb_devices)\n",
        "else:\n",
        "    save_callback = tf.keras.callbacks.ModelCheckpoint(save_filename,\n",
        "                                                       save_freq=20 * args.steps_per_epoch)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=args.lr), loss=losses, loss_weights=weights)\n",
        "\n",
        "# save starting weights\n",
        "model.save(save_filename.format(epoch=args.initial_epoch))\n",
        "\n",
        "model.fit(generator,\n",
        "         initial_epoch=args.initial_epoch,\n",
        "         epochs=args.epochs,\n",
        "         steps_per_epoch=args.steps_per_epoch,\n",
        "         callbacks=[save_callback],\n",
        "         verbose=1\n",
        "         )\n"
      ],
      "metadata": {
        "id": "HT0uQYO13s45"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}